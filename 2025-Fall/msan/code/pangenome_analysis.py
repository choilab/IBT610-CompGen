"""
L. plantarum ë²”ìœ ì „ì²´ ë¶„ì„ í†µí•© íŒŒì´í”„ë¼ì¸
- Roary ê¸°ë°˜ Pan-genome ë¶„ì„
- CompareM AAI ë¶„ì„
- Surfaceome, COG, BGC ë¶„ì„
- 12600K + RTX 4060 + 32GB RAM ìµœì í™”
"""

import os
import sys
import shutil
import subprocess
import time
import json
from pathlib import Path
from datetime import datetime

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.spatial.distance import squareform
from scipy.cluster.hierarchy import linkage, dendrogram
from Bio import SeqIO
import psutil
import warnings
warnings.filterwarnings('ignore')

# ==================== ê²½ë¡œ ì„¤ì • ====================
BASE_DIR = Path("/mnt/c/Users/AN/Desktop/gDrive/Study/Leacture/IBT610")
GFF_DIR = BASE_DIR / "gff"
FAA_DIR = BASE_DIR / "Prokka_faa/matched_prokka_faa"
ROARY_DIR = BASE_DIR / "Roary"
FNA_DIR = BASE_DIR / "Result_sample"
GBK_DIR = BASE_DIR / "Prokka_gbk"
OUTPUT_DIR = BASE_DIR / "output_analysis"
OUTPUT_DIR.mkdir(exist_ok=True)

# Galaxy eggNOG ê²°ê³¼ (ì„ íƒì )
COG_ORTHOLOGS_FILE = ROARY_DIR / "core_gene_orthologs.tsv"
COG_ANNOTATIONS_FILE = ROARY_DIR / "core_gene_annotations.tsv"

# ì‹œìŠ¤í…œ ì„¤ì •
CPU_THREADS = 16
MEMORY_LIMIT = 32
GROUP_SIZE = 25

# CompareM ì„¤ì •
COMPAREM_AAI_TIMEOUT = 1800  # 30ë¶„

# PyTorch GPU ê°ì§€
try:
    import torch
    GPU_AVAILABLE = torch.cuda.is_available()
    GPU_NAME = torch.cuda.get_device_name(0) if GPU_AVAILABLE else "Not detected"
except ImportError:
    GPU_AVAILABLE = False
    GPU_NAME = "PyTorch not installed"

print("="*80)
print("  L. plantarum ë²”ìœ ì „ì²´ ë¶„ì„ (12600K + RTX 4060)")
print("="*80)
print(f"âœ“ CPU: Intel 12600K ({CPU_THREADS}ìŠ¤ë ˆë“œ)")
print(f"âœ“ GPU: {GPU_NAME}")
print(f"âœ“ RAM: 32GB (ì œí•œ: {MEMORY_LIMIT}GB)")
print(f"âœ“ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# ==================== ê³µí†µ í•¨ìˆ˜ ====================
def check_memory():
    """ë©”ëª¨ë¦¬ ë¶€ì¡± ê²½ê³ """
    mem = psutil.virtual_memory()
    if mem.available / (1024**3) < 6:
        print("âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡±! ì‹¤í–‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        sys.exit(1)

def checkpoint_exists(step_name):
    """ì²´í¬í¬ì¸íŠ¸ í™•ì¸"""
    return (OUTPUT_DIR / f"checkpoint_{step_name}.flag").exists()

def mark_checkpoint(step_name):
    """ì²´í¬í¬ì¸íŠ¸ ìƒì„±"""
    with open(OUTPUT_DIR / f"checkpoint_{step_name}.flag", 'w') as f:
        f.write(f"Completed: {datetime.now()}\n")

# ==================== PART 1: Roary ë²”ìœ ì „ì²´ ë¶„ì„ ====================
def create_gff_mapping(gff_dir):
    """GFF3ì—ì„œ locus_tag â†’ gene_name ë§¤í•‘"""
    print("\n[1/8] GFF3 ë§¤í•‘ ìƒì„± (Surfaceomeìš©)...")
    check_memory()
    
    if checkpoint_exists("gff_mapping"):
        print("âœ“ ì´ë¯¸ ì™„ë£Œë¨ (ì²´í¬í¬ì¸íŠ¸)")
        return {}
    
    if not gff_dir.exists():
        print(f"âš ï¸ GFF ë””ë ‰í„°ë¦¬ ì—†ìŒ: {gff_dir}")
        return {}
    
    mapping = {}
    gff_files = list(gff_dir.glob("*.gff3")) + list(gff_dir.glob("*.gff"))
    
    if not gff_files:
        print("âš ï¸ GFF3 íŒŒì¼ ì—†ìŒ")
        return {}
    
    for idx, gff_file in enumerate(gff_files, 1):
        if idx % 50 == 0:
            print(f"  ì²˜ë¦¬ ì¤‘... {idx}/{len(gff_files)}")
        
        with open(gff_file, 'r') as f:
            for line in f:
                if line.startswith('#') or '\t' not in line:
                    continue
                
                parts = line.strip().split('\t')
                if len(parts) < 9:
                    continue
                
                attrs = parts[8]
                import re
                lt_match = re.search(r'locus_tag=([^;]+)', attrs)
                gene_match = re.search(r'gene=([^;]+)', attrs)
                
                if lt_match and gene_match:
                    mapping[lt_match.group(1)] = gene_match.group(1)
    
    print(f"âœ“ ë§¤í•‘ ì™„ë£Œ: {len(mapping):,}ê°œ")
    mark_checkpoint("gff_mapping")
    return mapping

def load_roary_data(roary_dir, output_dir):
    """Roary CSV ë¡œë“œ ë° CAR ë¶„ë¥˜"""
    print("\n[2/8] Roary ë°ì´í„° ë¡œë“œ (CAR ë¶„ë¥˜)...")
    check_memory()
    
    if checkpoint_exists("roary_load"):
        csv_file = output_dir / "roary_classified.csv"
        if csv_file.exists():
            print("âœ“ ì´ë¯¸ ì™„ë£Œë¨ (ì²´í¬í¬ì¸íŠ¸)")
            return pd.read_csv(csv_file), None
    
    csv_file = roary_dir / "Galaxy3670-[Roary on data 3663, data 3662, and others Gene Presence Absence].csv"
    
    if not csv_file.exists():
        print(f"âœ— Roary CSV ì—†ìŒ: {csv_file}")
        return None, None
    
    df = pd.read_csv(csv_file, low_memory=False)
    meta_cols = 14
    strain_cols = df.columns[meta_cols:]
    
    df['num_isolates'] = df[strain_cols].notna().sum(axis=1)
    df['frequency'] = (df['num_isolates'] / len(strain_cols)) * 100
    
    def categorize_gene(freq):
        if freq >= 99: return 'Core'
        elif freq >= 15: return 'Accessory'
        else: return 'Rare'
    
    df['category'] = df['frequency'].apply(categorize_gene)
    
    # CAR ë¶„í¬ ì‹œê°í™”
    car_counts = df['category'].value_counts()
    plt.figure(figsize=(8, 6))
    car_counts.plot(kind='bar', color=['green', 'orange', 'blue'])
    plt.title('CAR Classification')
    plt.xlabel('Category')
    plt.ylabel('Number of Genes')
    plt.tight_layout()
    plt.savefig(output_dir / '01_car_classification.png', dpi=300)
    plt.close()
    
    df.to_csv(output_dir / "roary_classified.csv", index=False)
    
    print(f"âœ“ ì „ì²´ ìœ ì „ì: {len(df):,}ê°œ | ê· ì£¼: {len(strain_cols)}ê°œ")
    print(f"âœ“ Core: {car_counts.get('Core', 0):,}ê°œ")
    print(f"âœ“ Accessory: {car_counts.get('Accessory', 0):,}ê°œ")
    print(f"âœ“ Rare: {car_counts.get('Rare', 0):,}ê°œ")
    
    mark_checkpoint("roary_load")
    return df, strain_cols

def analyze_heaps_law(df, strain_cols, output_dir):
    """Heaps' Law ê³„ì‚°"""
    print("\n[3/8] Heaps' Law ë¶„ì„...")
    check_memory()
    
    if checkpoint_exists("heaps_law"):
        print("âœ“ ì´ë¯¸ ì™„ë£Œë¨ (ì²´í¬í¬ì¸íŠ¸)")
        return None
    
    if len(strain_cols) < 5:
        print("âš ï¸ ê· ì£¼ ìˆ˜ ë¶€ì¡±")
        return None
    
    sample_size = min(30, len(strain_cols))
    pan_sizes, core_sizes = [], []
    sample_range = range(5, sample_size + 1, 5)
    
    for n in sample_range:
        temp_pan, temp_core = [], []
        for _ in range(10):
            subset_cols = np.random.choice(strain_cols, n, replace=False)
            subset = df[subset_cols]
            temp_pan.append(subset.notna().any(axis=1).sum())
            temp_core.append(subset.notna().all(axis=1).sum())
        
        pan_sizes.append(np.mean(temp_pan))
        core_sizes.append(np.mean(temp_core))
    
    log_n = np.log(sample_range)
    log_pan = np.log(pan_sizes)
    coeffs = np.polyfit(log_n, log_pan, 1)
    lambda_val = coeffs[0]
    
    plt.figure(figsize=(10, 5))
    plt.plot(sample_range, pan_sizes, 'o-', label='Pan-genome', linewidth=2)
    plt.plot(sample_range, core_sizes, 'o-', label='Core-genome', linewidth=2)
    plt.xlabel('Number of Genomes')
    plt.ylabel('Number of Genes')
    plt.title(f'Heaps Law (Î» = {lambda_val:.3f})')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.savefig(output_dir / '02_heaps_law.png', dpi=300)
    plt.close()
    
    print(f"âœ“ Heaps' Law Î» = {lambda_val:.3f}")
    mark_checkpoint("heaps_law")
    return lambda_val

def analyze_mash(fna_dir, output_dir):
    """Mash clustering"""
    print("\n[4/8] Mash Clustering...")
    check_memory()
    
    if checkpoint_exists("mash"):
        print("âœ“ ì´ë¯¸ ì™„ë£Œë¨ (ì²´í¬í¬ì¸íŠ¸)")
        return None
    
    if subprocess.run(["which", "mash"], capture_output=True).returncode != 0:
        print("âš ï¸ Mash ë¯¸ì„¤ì¹˜")
        return None
    
    fna_files = list(fna_dir.glob("*.fna")) + list(fna_dir.glob("*.fasta"))
    if len(fna_files) < 2:
        print("âš ï¸ FASTA íŒŒì¼ ë¶€ì¡±")
        return None
    
    sketch_file = output_dir / "mash_sketch.msh"
    cmd = f"mash sketch -o {sketch_file.with_suffix('')} -p {CPU_THREADS} -s 10000 " + " ".join([str(f) for f in fna_files[:50]])
    
    try:
        subprocess.run(cmd, shell=True, check=True, capture_output=True)
        
        dist_file = output_dir / "mash_distances.tsv"
        cmd = f"mash dist {sketch_file} {sketch_file} > {dist_file}"
        subprocess.run(cmd, shell=True, check=True)
        
        # ì‹œê°í™”
        distances = []
        with open(dist_file, 'r') as f:
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) >= 3:
                    distances.append({
                        'genome1': Path(parts[0]).stem,
                        'genome2': Path(parts[1]).stem,
                        'distance': float(parts[2])
                    })
        
        if distances:
            dist_df = pd.DataFrame(distances)
            pivot_df = dist_df.pivot(index='genome1', columns='genome2', values='distance')
            dist_matrix = squareform(pivot_df.fillna(0).values)
            
            Z = linkage(dist_matrix, method='ward')
            
            plt.figure(figsize=(12, 8))
            dendrogram(Z, labels=pivot_df.index, leaf_rotation=90)
            plt.title('Mash-based Genome Clustering')
            plt.tight_layout()
            plt.savefig(output_dir / '03_mash_clustering.png', dpi=300)
            plt.close()
            
            print("âœ“ Mash clustering ì™„ë£Œ")
            mark_checkpoint("mash")
            return dist_df
    except Exception as e:
        print(f"âš ï¸ Mash ì‹¤íŒ¨: {e}")
    
    return None

def analyze_cog(roary_dir, output_dir):
    """Galaxy eggNOG ê²°ê³¼ í™œìš© COG ë¶„ì„"""
    print("\n[5/8] COG ë¶„ì„ (Galaxy ê²°ê³¼ í™œìš©)...")
    check_memory()
    
    if checkpoint_exists("cog"):
        print("âœ“ ì´ë¯¸ ì™„ë£Œë¨ (ì²´í¬í¬ì¸íŠ¸)")
        return None
    
    if not COG_ORTHOLOGS_FILE.exists():
        print(f"âš ï¸ COG orthologs íŒŒì¼ ì—†ìŒ: {COG_ORTHOLOGS_FILE}")
        print("    â†’ COG ë¶„ì„ ìƒëµ (Galaxyì—ì„œ ì™„ë£Œ í›„ ë‹¤ì‹œ ì‹¤í–‰)")
        return None
    
    try:
        ortho_df = pd.read_csv(COG_ORTHOLOGS_FILE, sep='\t')
        print(f"  â†’ Orthologs: {len(ortho_df)}ê°œ")
        
        cog_col = None
        for col in ortho_df.columns:
            if 'cog' in col.lower() or 'category' in col.lower():
                cog_col = col
                break
        
        if cog_col is None:
            cog_col = ortho_df.columns[1]
        
        cog_counts = ortho_df[cog_col].value_counts().head(15)
        
        plt.figure(figsize=(14, 7))
        cog_counts.plot(kind='bar', color='steelblue')
        plt.title('COG Category Distribution')
        plt.xlabel('COG Category')
        plt.ylabel('Number of Genes')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(output_dir / '04_cog_analysis.png', dpi=300)
        plt.close()
        
        print(f"âœ“ COG ë¶„ì„ ì™„ë£Œ: {len(cog_counts)}ê°œ ì¹´í…Œê³ ë¦¬")
        mark_checkpoint("cog")
        return ortho_df
        
    except Exception as e:
        print(f"âš ï¸ COG íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None

def analyze_surfaceome(mapping, df, strain_cols, faa_dir, output_dir):
    """Surfaceome ë¶„ì„"""
    print("\n[6/8] Surfaceome ë¶„ì„...")
    check_memory()
    
    if checkpoint_exists("surfaceome"):
        csv_file = output_dir / 'surfaceome_predictions.csv'
        if csv_file.exists():
            print("âœ“ ì´ë¯¸ ì™„ë£Œë¨ (ì²´í¬í¬ì¸íŠ¸)")
            return pd.read_csv(csv_file)
    
    if not mapping:
        print("âš ï¸ ë§¤í•‘ í…Œì´ë¸” ì—†ìŒ")
        return None
    
    gene_map = dict(zip(df['Gene'], df['category']))
    faa_files = list(faa_dir.glob("*.faa")) + list(faa_dir.glob("*.fasta"))
    
    if not faa_files:
        print("âš ï¸ FAA íŒŒì¼ ì—†ìŒ")
        return None
    
    print(f"  {len(faa_files)}ê°œ íŒŒì¼ ë¶„ì„ ì¤‘...")
    
    class SurfaceomeAnalyzer:
        @staticmethod
        def analyze(seq):
            seq_str = str(seq).upper()
            if len(seq_str) < 30:
                return 'Intracellular'
            
            n_term = seq_str[:30]
            has_sp = sum(1 for aa in n_term if aa in 'LVIAFWM') >= 20
            
            c_term = seq_str[-60:]
            has_lpxtg = 'LP' in c_term and 'TG' in c_term
            
            hydrophobic = sum(1 for aa in seq_str if aa in 'LVIAFWM')
            is_tm = hydrophobic / len(seq_str) > 0.4
            
            if has_lpxtg and has_sp: return 'Cell_wall'
            elif has_sp: return 'Secreted'
            elif is_tm: return 'Membrane'
            else: return 'Intracellular'
    
    results = []
    for idx, faa_file in enumerate(faa_files, 1):
        if idx % 30 == 0:
            print(f"  ì²˜ë¦¬ ì¤‘... {idx}/{len(faa_files)}")
            check_memory()
        
        strain_name = faa_file.stem
        
        for record in SeqIO.parse(faa_file, "fasta"):
            locus_tag = record.id.split()[0]
            gene_name = mapping.get(locus_tag)
            
            if gene_name and gene_name in gene_map:
                loc = SurfaceomeAnalyzer.analyze(record.seq)
                results.append({
                    'Protein_ID': locus_tag,
                    'Strain': strain_name,
                    'Gene': gene_name,
                    'Category': gene_map[gene_name],
                    'Localization': loc
                })
    
    if not results:
        return None
    
    res_df = pd.DataFrame(results)
    res_df.to_csv(output_dir / 'surfaceome_predictions.csv', index=False)
    
    surface_df = res_df[res_df['Localization'] != 'Intracellular']
    
    if not surface_df.empty:
        ct = pd.crosstab(surface_df['Category'], surface_df['Localization'])
        ct_pct = ct.div(ct.sum(axis=1), axis=0) * 100
        
        plt.figure(figsize=(10, 6))
        ct_pct.plot(kind='bar', stacked=True, colormap='viridis', ax=plt.gca())
        plt.title('Surfaceome Distribution by CAR Category')
        plt.xlabel('Gene Category')
        plt.ylabel('Percentage')
        plt.legend(title='Localization')
        plt.tight_layout()
        plt.savefig(output_dir / '05_surfaceome.png', dpi=300)
        plt.close()
        
        print(f"âœ“ í‘œë©´ ë‹¨ë°±ì§ˆ: {len(surface_df):,}ê°œ")
        mark_checkpoint("surfaceome")
        return res_df
    
    return None

# ==================== PART 2: CompareM AAI ë¶„ì„ ====================
def find_comparem():
    """CompareM ë„êµ¬ íƒì§€"""
    conda_prefix = os.environ.get('CONDA_PREFIX')
    if conda_prefix:
        possible_path = os.path.join(conda_prefix, 'bin', 'comparem')
        if os.path.exists(possible_path):
            return possible_path
    
    comparem_path = shutil.which('comparem')
    if comparem_path:
        return comparem_path
    
    print("âš ï¸ CompareMì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    print("   ì„¤ì¹˜: conda install -c bioconda comparem")
    return None

def run_comparem_aai(faa_dir, output_dir):
    """CompareM AAI ë¶„ì„"""
    print("\n[7/8] CompareM AAI ë¶„ì„...")
    check_memory()
    
    if checkpoint_exists("comparem_aai"):
        print("âœ“ ì´ë¯¸ ì™„ë£Œë¨ (ì²´í¬í¬ì¸íŠ¸)")
        return True
    
    comparem_path = find_comparem()
    if not comparem_path:
        return False
    
    faa_files = list(faa_dir.glob("*.fasta"))
    if not faa_files:
        print("âš ï¸ FAA íŒŒì¼ ì—†ìŒ")
        return False
    
    # ê·¸ë£¹ ë¶„í• 
    groups = [faa_files[i:i+GROUP_SIZE] 
             for i in range(0, len(faa_files), GROUP_SIZE)]
    
    print(f"  {len(faa_files)}ê°œ íŒŒì¼ â†’ {len(groups)}ê°œ ê·¸ë£¹")
    
    all_results = []
    
    for gi, files in enumerate(groups, 1):
        group_dir = output_dir / f"aai_group_{gi}"
        group_dir.mkdir(exist_ok=True)
        
        # íŒŒì¼ ë³µì‚¬
        for fname in files:
            shutil.copy2(fname, group_dir / fname.name)
        
        aai_out = group_dir / "output_aai"
        aai_out.mkdir(exist_ok=True)
        
        print(f"\n  [ê·¸ë£¹ {gi}/{len(groups)}] ì‹¤í–‰ ì¤‘...")
        
        cmd = [
            comparem_path, "aai_wf",
            "--proteins", "--file_ext", "fasta",
            "--cpus", str(CPU_THREADS),
            str(group_dir), str(aai_out)
        ]
        
        try:
            subprocess.run(cmd, timeout=COMPAREM_AAI_TIMEOUT, check=True)
            
            result_file = aai_out / "aai" / "aai_summary.tsv"
            if result_file.exists():
                df = pd.read_csv(result_file, sep="\t")
                all_results.append(df)
                print(f"    âœ“ ì„±ê³µ")
        except subprocess.TimeoutExpired:
            print(f"    âœ— íƒ€ì„ì•„ì›ƒ")
        except Exception as e:
            print(f"    âœ— ì˜¤ë¥˜: {e}")
    
    if all_results:
        merged_df = pd.concat(all_results, ignore_index=True)
        merged_df.to_csv(output_dir / "aai_summary_merged.tsv", sep='\t', index=False)
        print(f"\nâœ“ AAI ë¶„ì„ ì™„ë£Œ: {len(merged_df)}ê°œ ë¹„êµ")
        mark_checkpoint("comparem_aai")
        return True
    
    return False

# ==================== ë©”ì¸ ì‹¤í–‰ ====================
def main():
    start_time = time.time()
    
    print("\nì‹œì‘ ë¶„ì„...")
    print(f"ë©”ëª¨ë¦¬: {psutil.virtual_memory().available / (1024**3):.1f}GB ì‚¬ìš© ê°€ëŠ¥")
    
    # 1. GFF ë§¤í•‘
    mapping = create_gff_mapping(GFF_DIR)
    
    # 2. Roary ë°ì´í„°
    roary_df, strain_cols = load_roary_data(ROARY_DIR, OUTPUT_DIR)
    if roary_df is None:
        sys.exit(1)
    
    # 3. Heaps' Law
    lambda_val = analyze_heaps_law(roary_df, strain_cols, OUTPUT_DIR)
    
    # 4. Mash
    mash_df = analyze_mash(FNA_DIR, OUTPUT_DIR)
    
    # 5. COG
    cog_df = analyze_cog(ROARY_DIR, OUTPUT_DIR)
    
    # 6. Surfaceome
    surface_df = analyze_surfaceome(mapping, roary_df, strain_cols, FAA_DIR, OUTPUT_DIR)
    
    # 7. CompareM AAI
    aai_success = run_comparem_aai(FAA_DIR, OUTPUT_DIR)
    
    # ê²°ê³¼ ìš”ì•½
    elapsed = time.time() - start_time
    print("\n" + "="*80)
    print("  ë¶„ì„ ì™„ë£Œ ìš”ì•½")
    print("="*80)
    print(f"âœ“ Pangenome: {len(roary_df):,} ìœ ì „ì")
    print(f"âœ“ Heaps' Law Î»: {lambda_val:.3f}" if lambda_val else "âœ— Heaps' Law: ìƒëµ")
    print(f"âœ“ Mash: {'ì™„ë£Œ' if mash_df is not None else 'ìƒëµ'}")
    print(f"âœ“ COG: {'ì™„ë£Œ' if cog_df is not None else 'ìƒëµ'}")
    print(f"âœ“ Surfaceome: {len(surface_df) if surface_df is not None else 0:,} ë‹¨ë°±ì§ˆ")
    print(f"âœ“ AAI: {'ì™„ë£Œ' if aai_success else 'ìƒëµ'}")
    print(f"\nâ±ï¸ ì‹¤í–‰ ì‹œê°„: {elapsed/60:.1f}ë¶„")
    print(f"ğŸ“ ê²°ê³¼: {OUTPUT_DIR}")

if __name__ == "__main__":
    main()
