"""
Lactobacillus plantarum AAI ë¶„ì„ íŒŒì´í”„ë¼ì¸ (12600K ìµœì í™”)
- ìŠ¤ë ˆë“œ ê²½í•© ë°©ì§€
- íƒ€ì„ì•„ì›ƒ 30ë¶„ ì„¤ì •
- ì§„í–‰ ìƒí™© ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
"""

import os
import sys
import shutil
import subprocess
import time
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.spatial.distance import squareform
from scipy.cluster.hierarchy import linkage, dendrogram

# ========== ì„¤ì • ==========
# ì‚¬ìš©ì ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”!)
FNA_DIR = r"C:\Users\AN\Desktop\gDrive\Study\Leacture\IBT610\Result_sample"
FAA_DIR = r"C:\Users\AN\Desktop\gDrive\Study\Leacture\IBT610\Prokka_faa"
OUTPUT_NAME = "output_lplantarum_aai"

# ì‘ì—… ì„¤ì •
GROUP_SIZE = 25
CPU_THREADS = 16
ANI_THRESHOLD = 95
TIMEOUT_SECONDS = 2700   # 30ë¶„ íƒ€ì„ì•„ì›ƒ ì¶”ê°€

# MLST ì„¤ì • (ì„ íƒì )
MLST_SCHEME = "lplantarum"
MLST_TOOL = "mlst"  # 'mlst' ë˜ëŠ” Noneìœ¼ë¡œ ë¹„í™œì„±í™”

# Linux ê²½ë¡œ (WSL)
FNA_DIR_LINUX = "/mnt/c/Users/AN/Desktop/gDrive/Study/Leacture/IBT610/Result_sample"
FAA_DIR_LINUX = "/mnt/c/Users/AN/Desktop/gDrive/Study/Leacture/IBT610/Prokka_faa"

# ========== ê²½ë¡œ ë³€í™˜ ==========
def is_wsl():
    """WSL í™˜ê²½ì¸ì§€ í™•ì¸"""
    try:
        with open('/proc/version', 'r') as f:
            return 'microsoft' in f.read().lower()
    except:
        return False

def get_working_paths():
    """ìš´ì˜ì²´ì œì— ë§ëŠ” ê²½ë¡œ ë°˜í™˜"""
    if is_wsl() or os.path.exists('/mnt/c'):
        return FNA_DIR_LINUX, FAA_DIR_LINUX
    else:
        return FNA_DIR, FAA_DIR

# ========== ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ ==========
def checkpoint_file(step_name, output_dir):
    """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    return os.path.join(output_dir, f"checkpoint_{step_name}.flag")

def is_step_done(step_name, output_dir):
    """í•´ë‹¹ ë‹¨ê³„ê°€ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
    cp_file = checkpoint_file(step_name, output_dir)
    return os.path.exists(cp_file)

def mark_step_done(step_name, output_dir):
    """ë‹¨ê³„ ì™„ë£Œ í‘œì‹œ"""
    cp_file = checkpoint_file(step_name, output_dir)
    with open(cp_file, 'w') as f:
        f.write(f"Completed at: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")

# ========== FASTA íŒŒì¼ ê²€ì¦ ==========
def validate_fasta_file(file_path):
    """FASTA íŒŒì¼ í˜•ì‹ ê²€ì¦"""
    try:
        with open(file_path, 'r') as f:
            first_line = f.readline().strip()
            if not first_line.startswith('>'):
                return False, f"ì²« ì¤„ì´ '>'ë¡œ ì‹œì‘í•˜ì§€ ì•ŠìŒ: {first_line[:50]}"
            
            second_line = f.readline().strip()
            if not second_line:
                return False, "ë‘ ë²ˆì§¸ ì¤„ì´ ë¹„ì–´ìˆìŒ"
            
            if len(second_line) < 10:
                return False, f"ì„œì—´ì´ ë„ˆë¬´ ì§§ìŒ: {len(second_line)}bp"
        
        file_size = os.path.getsize(file_path)
        if file_size < 100:
            return False, f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {file_size}bytes"
            
        return True, "OK"
    except Exception as e:
        return False, f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}"

# ========== ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ í™•ì¸ ==========
def get_processed_files(work_dir):
    """ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ í™•ì¸"""
    processed = set()
    if not os.path.exists(work_dir):
        return processed
        
    for d in os.listdir(work_dir):
        if d.startswith("group_"):
            group_dir = os.path.join(work_dir, d)
            # ê²°ê³¼ íŒŒì¼ í™•ì¸
            result_file = os.path.join(group_dir, "output_aai", "aai", "aai_summary.tsv")
            if os.path.exists(result_file) and os.path.getsize(result_file) > 100:
                # ì´ ê·¸ë£¹ì˜ ì…ë ¥ íŒŒì¼ë“¤ ëª©ë¡
                for f in os.listdir(group_dir):
                    if f.endswith('.fasta'):
                        processed.add(f)
    return processed

# ========== MLST ë„êµ¬ ìë™ íƒì§€ (ì•ˆì „ ë²„ì „) ==========
def find_mlst():
    """MLST ë„êµ¬ ìë™ íƒì§€ - í™˜ê²½ ì¶©ëŒ ì‹œ None ë°˜í™˜"""
    print("\n" + "="*70)
    print("[MLST ë„êµ¬ íƒì§€]")
    print("="*70)
    
    # MLST ë¹„í™œì„±í™” ì„¤ì •
    if MLST_TOOL is None:
        print("âš ï¸  MLST_TOOLì´ Noneìœ¼ë¡œ ì„¤ì •ë¨ â†’ MLST ë¶„ì„ ê±´ë„ˆëœ€")
        return None
    
    # which ëª…ë ¹ì–´ë¡œ ì°¾ê¸°
    try:
        result = subprocess.run(['which', 'mlst'], capture_output=True, text=True, check=True)
        mlst_path = result.stdout.strip()
        if os.path.exists(mlst_path):
            print(f"âœ“ MLST ë°œê²¬: {mlst_path}")
            
            # ìŠ¤í‚´ í™•ì¸
            try:
                scheme_result = subprocess.run(['mlst', '--list'], capture_output=True, text=True, timeout=10)
                available_schemes = scheme_result.stdout.strip().split('\n')
                if MLST_SCHEME in available_schemes:
                    print(f"âœ“ ìŠ¤í‚´ '{MLST_SCHEME}' ì‚¬ìš© ê°€ëŠ¥")
                    return mlst_path
                else:
                    print(f"âš ï¸  ìŠ¤í‚´ '{MLST_SCHEME}' ì—†ìŒ")
                    return None
            except:
                print("âš ï¸  MLST ìŠ¤í‚´ í™•ì¸ ì‹¤íŒ¨")
                return None
    except:
        print("âŒ MLST ë„êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("   ì„¤ì¹˜ ë°©ë²• (Python 3.13 ì¶©ëŒ ì£¼ì˜):")
        print("   â†’ ë³„ë„ í™˜ê²½ ìƒì„±: conda create -n mlst_env python=3.11 mlst")
        print("   â†’ í™œì„±í™”: conda activate mlst_env")
        print("   â†’ ìˆ˜ë™ ì‹¤í–‰: mlst --scheme lplantarum *.fasta")
        return None

# ========== 1ë‹¨ê³„: Prokka-FNA ë§¤ì¹­ ==========
def match_prokka_fna(output_dir):
    """Prokka .fasta íŒŒì¼ ë§¤ì¹­ (í†µí•© íŒŒì¼ë§Œ)"""
    step_name = "01_matching"
    if is_step_done(step_name, output_dir):
        match_file = os.path.join(output_dir, "matched_samples.tsv")
        if os.path.exists(match_file):
            return pd.read_csv(match_file, sep="\t"), *get_working_paths()
        return None, None, None
    
    print("\n" + "="*70)
    print("[1ë‹¨ê³„] Prokka-FNA ë§¤ì¹­ (í†µí•© íŒŒì¼ë§Œ)")
    print("="*70)
    
    fna_dir, faa_dir = get_working_paths()
    
    if not os.path.exists(fna_dir):
        print(f"âŒ FNA ë””ë ‰í† ë¦¬ ì—†ìŒ: {fna_dir}")
        return None, None, None
    
    if not os.path.exists(faa_dir):
        print(f"âŒ FAA ë””ë ‰í† ë¦¬ ì—†ìŒ: {faa_dir}")
        return None, None, None
    
    fna_files = [f for f in os.listdir(fna_dir) if f.endswith('.fna')]
    faa_files = [f for f in os.listdir(faa_dir) if f.endswith('.fasta') and '_part' not in f]
    
    print(f"\nğŸ“‚ íŒŒì¼ íƒìƒ‰:")
    print(f"   - FNA íŒŒì¼: {len(fna_files)}ê°œ")
    print(f"   - FAA íŒŒì¼ (í†µí•©ë§Œ): {len(faa_files)}ê°œ")
    
    # GCF/GCA ID ì¶”ì¶œ
    fna_gcf_map = {}
    for f in fna_files:
        if f.startswith('GCF_') or f.startswith('GCA_'):
            gcf_id = f.split('.')[0]
            fna_gcf_map[gcf_id] = f
    
    faa_gcf_map = {}
    for f in faa_files:
        if f.startswith('GCF_') or f.startswith('GCA_'):
            prefix = f.replace('.fasta', '')
            faa_gcf_map[prefix] = f
    
    print(f"   - FNA ID: {len(fna_gcf_map)}ê°œ")
    print(f"   - FAA ID: {len(faa_gcf_map)}ê°œ")
    
    # ë§¤ì¹­ ë° ê²€ì¦
    matched = []
    validation_errors = []
    for fna_gcf, fna_file in fna_gcf_map.items():
        if fna_gcf in faa_gcf_map:
            faa_file = faa_gcf_map[fna_gcf]
            faa_path = os.path.join(faa_dir, faa_file)
            
            if not os.path.exists(faa_path):
                validation_errors.append(f"{faa_file}: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
                continue
            
            is_valid, msg = validate_fasta_file(faa_path)
            if is_valid:
                matched.append({
                    'GCF_ID': fna_gcf,
                    'FNA_file': fna_file,
                    'FAA_file': faa_file
                })
            else:
                validation_errors.append(f"{faa_file}: {msg}")
    
    if validation_errors:
        print(f"\nâš ï¸  ê²€ì¦ ì˜¤ë¥˜ ({len(validation_errors)}ê°œ):")
        for err in validation_errors[:5]:
            print(f"   - {err}")
        if len(validation_errors) > 5:
            print(f"   ... ì™¸ {len(validation_errors)-5}ê°œ")
    
    if not matched:
        print("\nâŒ ë§¤ì¹­ë˜ê³  ê²€ì¦ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return None, None, None
    
    matched_df = pd.DataFrame(matched)
    os.makedirs(output_dir, exist_ok=True)
    
    match_file = os.path.join(output_dir, "matched_samples.tsv")
    matched_df.to_csv(match_file, sep="\t", index=False)
    
    print(f"\nâœ… ë§¤ì¹­ ì™„ë£Œ: {len(matched)}ê°œ íŒŒì¼")
    print(f"ğŸ“„ ì €ì¥: {match_file}")
    
    mark_step_done(step_name, output_dir)
    return matched_df, fna_dir, faa_dir

# ========== 2ë‹¨ê³„: FAA íŒŒì¼ ë³µì‚¬ ==========
def prepare_faa_working_directory(matched_df, faa_dir, fna_dir):
    """ë§¤ì¹­ëœ FAA íŒŒì¼ë§Œ ì‘ì—… ë””ë ‰í† ë¦¬ì— ë³µì‚¬"""
    output_dir = os.path.join(fna_dir, OUTPUT_NAME)
    step_name = "02_copy_faa"
    if is_step_done(step_name, output_dir):
        work_dir = os.path.join(faa_dir, "matched_prokka_faa")
        return work_dir if os.path.exists(work_dir) else None
    
    print("\n" + "="*70)
    print("[2ë‹¨ê³„] ë§¤ì¹­ëœ FAA íŒŒì¼ ë³µì‚¬")
    print("="*70)
    
    work_dir = os.path.join(faa_dir, "matched_prokka_faa")
    os.makedirs(work_dir, exist_ok=True)
    
    print(f"\nğŸ“ ì‘ì—… ë””ë ‰í† ë¦¬: {work_dir}")
    print(f"ğŸ”„ {len(matched_df)}ê°œ íŒŒì¼ ë³µì‚¬ ì¤‘...")
    
    copied = 0
    for idx, row in matched_df.iterrows():
        src = os.path.join(faa_dir, row['FAA_file'])
        dst = os.path.join(work_dir, row['FAA_file'])
        
        if os.path.exists(src):
            if not os.path.exists(dst):
                shutil.copy2(src, dst)
            copied += 1
        else:
            print(f"   âš ï¸  íŒŒì¼ ì—†ìŒ: {src}")
    
    print(f"âœ… ë³µì‚¬ ì™„ë£Œ: {copied}ê°œ íŒŒì¼")
    mark_step_done(step_name, output_dir)
    
    return work_dir

# ========== CompareM ìë™ íƒì§€ ==========
def find_comparem():
    """CompareM ì‹¤í–‰ íŒŒì¼ ê²½ë¡œ ìë™ íƒì§€"""
    print("\n" + "="*70)
    print("[CompareM íƒì§€]")
    print("="*70)
    
    # 1. í˜„ì¬ conda í™˜ê²½
    conda_prefix = os.environ.get('CONDA_PREFIX')
    if conda_prefix:
        possible_path = os.path.join(conda_prefix, 'bin', 'comparem')
        if os.path.exists(possible_path):
            print(f"âœ“ ë°œê²¬ (conda): {possible_path}")
            return possible_path
    
    # 2. which ëª…ë ¹ì–´
    try:
        result = subprocess.run(['which', 'comparem'], capture_output=True, text=True, check=True)
        comparem_path = result.stdout.strip()
        if os.path.exists(comparem_path):
            print(f"âœ“ ë°œê²¬: {comparem_path}")
            return comparem_path
    except:
        pass
    
    # 3. PATHì—ì„œ ì°¾ê¸°
    comparem_path = shutil.which('comparem')
    if comparem_path:
        print(f"âœ“ ë°œê²¬ (PATH): {comparem_path}")
        return comparem_path
    
    print("âŒ CompareMì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    print("   ì„¤ì¹˜: conda install -c bioconda comparem")
    return None

# ========== 3ë‹¨ê³„: ê·¸ë£¹ë³„ AAI ë¶„ì„ (ê°œì„ ) ==========
def group_aai_analysis(comparem_path, work_dir, output_dir):
    """ê·¸ë£¹ë³„ AAI ë¶„ì„ (íƒ€ì„ì•„ì›ƒ + ìŠ¤ë ˆë“œ ê²½í•© ë°©ì§€)"""
    step_name = "03_aai_analysis"
    if is_step_done(step_name, output_dir):
        return True, work_dir
    
    print("\n" + "="*70)
    print("[3ë‹¨ê³„] ê·¸ë£¹ë³„ AAI ë¶„ì„")
    print("="*70)
    
    faa_files = [f for f in os.listdir(work_dir) if f.endswith('.fasta')]
    total_files = len(faa_files)
    
    if total_files == 0:
        print("âŒ ì‘ì—… ë””ë ‰í† ë¦¬ì— FAA íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return False, work_dir
    
    processed_files = get_processed_files(work_dir)
    remaining_files = [f for f in faa_files if f not in processed_files]
    
    print(f"\nğŸ“Š ì²˜ë¦¬ ìƒíƒœ:")
    print(f"   - ì „ì²´ íŒŒì¼: {total_files}ê°œ")
    print(f"   - ì´ë¯¸ ì²˜ë¦¬ë¨: {len(processed_files)}ê°œ")
    print(f"   - ë‚¨ì€ íŒŒì¼: {len(remaining_files)}ê°œ")
    
    if not remaining_files:
        print("âœ… ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        mark_step_done(step_name, output_dir)
        return True, work_dir
    
    groups = [remaining_files[i:i+GROUP_SIZE] for i in range(0, len(remaining_files), GROUP_SIZE)]
    total_groups = len(groups)
    
    print(f"\nğŸ“Š ë¶„ì„ ì„¤ì •:")
    print(f"   - ì²˜ë¦¬í•  íŒŒì¼: {len(remaining_files)}ê°œ")
    print(f"   - ê·¸ë£¹ ìˆ˜: {total_groups}ê°œ")
    print(f"   - ê·¸ë£¹ í¬ê¸°: {GROUP_SIZE}ê°œ")
    print(f"   - CPU ìŠ¤ë ˆë“œ: {CPU_THREADS}ê°œ")
    print(f"   - íƒ€ì„ì•„ì›ƒ: {TIMEOUT_SECONDS//60}ë¶„")
    
    success_count = 0
    failed_groups = []
    
    for gi, files in enumerate(groups, 1):
        group_dir = os.path.join(work_dir, f"group_{gi}")
        os.makedirs(group_dir, exist_ok=True)
        
        # íŒŒì¼ ë³µì‚¬
        for fname in files:
            src = os.path.join(work_dir, fname)
            dst = os.path.join(group_dir, fname)
            if not os.path.exists(dst):
                shutil.copy2(src, dst)
        
        aai_out = os.path.join(group_dir, "output_aai")
        os.makedirs(aai_out, exist_ok=True)
        
        print(f"\n[ê·¸ë£¹ {gi}/{total_groups}] ì‹¤í–‰ ì¤‘...")
        print(f"   â”œâ”€â”€ íŒŒì¼: {len(files)}ê°œ (ì²« íŒŒì¼: {files[0]})")
        print(f"   â”œâ”€â”€ CPU: {CPU_THREADS} threads")
        ts = time.time()
        
        cmd = [
            comparem_path, "aai_wf",
            "--proteins", "--file_ext", "fasta",
            "--cpus", str(CPU_THREADS),
            group_dir, aai_out
        ]
        
        try:
            # CPU ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ ì‹œì‘
            monitor_process = subprocess.Popen([
                'bash', '-c', 
                f'while true; do top -bn1 | grep "Cpu(s)"; sleep 10; done'
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=TIMEOUT_SECONDS
            )
            
            # ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
            monitor_process.terminate()
            
            result_file = os.path.join(aai_out, "aai", "aai_summary.tsv")
            if os.path.exists(result_file) and os.path.getsize(result_file) > 100:
                te = time.time()
                elapsed_min = (te - ts) / 60
                print(f"   â””â”€â”€ âœ“ ì„±ê³µ ({elapsed_min:.1f}ë¶„)")
                success_count += 1
            else:
                print(f"   â””â”€â”€ âš ï¸  ê²°ê³¼ ì—†ìŒ (ì½”ë“œ: {result.returncode})")
                failed_groups.append(gi)
                
        except subprocess.TimeoutExpired:
            monitor_process.terminate()
            print(f"   â””â”€â”€ âœ— íƒ€ì„ì•„ì›ƒ ({TIMEOUT_SECONDS//60}ë¶„ ì´ˆê³¼)")
            failed_groups.append(gi)
        except Exception as e:
            try:
                monitor_process.terminate()
            except:
                pass
            print(f"   â””â”€â”€ âœ— ì˜¤ë¥˜: {str(e)}")
            failed_groups.append(gi)
    
    print(f"\n{'â”€'*70}")
    print(f"ğŸ“Š ê·¸ë£¹ë³„ ë¶„ì„: {success_count}/{total_groups} ì„±ê³µ")
    if failed_groups:
        print(f"âš ï¸  ì‹¤íŒ¨ ê·¸ë£¹: {', '.join(map(str, failed_groups))}")
    print(f"{'â”€'*70}")
    
    if success_count > 0:
        mark_step_done(step_name, output_dir)
    
    return success_count > 0, work_dir

# ========== 4ë‹¨ê³„: ê²°ê³¼ í†µí•© ==========
def merge_aai_results(work_dir, output_dir):
    """AAI ê²°ê³¼ í†µí•©"""
    step_name = "04_merge_results"
    if is_step_done(step_name, output_dir):
        merge_file = os.path.join(output_dir, "aai_summary_merged.tsv")
        if os.path.exists(merge_file):
            return pd.read_csv(merge_file, sep="\t")
        return None
    
    print("\n" + "="*70)
    print("[4ë‹¨ê³„] AAI ê²°ê³¼ í†µí•©")
    print("="*70)
    
    result_files = []
    for d in os.listdir(work_dir):
        if d.startswith("group_"):
            result_file = os.path.join(work_dir, d, "output_aai", "aai", "aai_summary.tsv")
            if os.path.exists(result_file) and os.path.getsize(result_file) > 100:
                result_files.append(result_file)
    
    if not result_files:
        print("âŒ í†µí•©í•  ê²°ê³¼ ì—†ìŒ")
        return None
    
    print(f"\nğŸ“‚ ë°œê²¬ëœ ê²°ê³¼: {len(result_files)}ê°œ ê·¸ë£¹")
    
    aai_parts = []
    for idx, rf in enumerate(result_files, 1):
        try:
            df = pd.read_csv(rf, sep="\t")
            df.columns = [c.strip() for c in df.columns]
            
            g1_col = [c for c in df.columns if any(x in c for x in ["Genome A", "genome1", "Genome_A"])][0]
            g2_col = [c for c in df.columns if any(x in c for x in ["Genome B", "genome2", "Genome_B"])][0]
            aai_col = [c for c in df.columns if any(x in c for x in ["Mean AAI", "aai", "AAI"])][0]
            
            df2 = df[[g1_col, g2_col, aai_col]].copy()
            df2.columns = ["Genome_A", "Genome_B", "AAI"]
            aai_parts.append(df2)
            
        except Exception as e:
            print(f"[{idx}] âœ— ì‹¤íŒ¨: {str(e)}")
    
    if not aai_parts:
        print("\nâŒ ì½ì„ ìˆ˜ ìˆëŠ” ê²°ê³¼ ì—†ìŒ")
        return None
    
    all_aai = pd.concat(aai_parts, axis=0).drop_duplicates()
    merge_file = os.path.join(output_dir, "aai_summary_merged.tsv")
    all_aai.to_csv(merge_file, sep="\t", index=False)
    
    print(f"\nâœ… í†µí•© ì™„ë£Œ:")
    print(f"   - ì´ ë¹„êµ ìŒ: {len(all_aai):,}ê°œ")
    print(f"   - íŒŒì¼: {merge_file}")
    print(f"   - AAI ë²”ìœ„: {all_aai['AAI'].min():.2f}% ~ {all_aai['AAI'].max():.2f}%")
    
    mark_step_done(step_name, output_dir)
    return all_aai

# ========== 5ë‹¨ê³„: ë§¤íŠ¸ë¦­ìŠ¤ ë° ì‹œê°í™” ==========
def build_matrix_and_visualize(aai_df, output_dir):
    """ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ë° ì‹œê°í™”"""
    step_name = "05_matrix_viz"
    if is_step_done(step_name, output_dir):
        mat_file = os.path.join(output_dir, "aai_matrix.csv")
        if os.path.exists(mat_file):
            return pd.read_csv(mat_file, index_col=0)
        return None
    
    print("\n" + "="*70)
    print("[5ë‹¨ê³„] ë§¤íŠ¸ë¦­ìŠ¤ ë° ì‹œê°í™”")
    print("="*70)
    
    names = sorted(set(aai_df["Genome_A"]).union(set(aai_df["Genome_B"])))
    print(f"\nğŸ“Š ë§¤íŠ¸ë¦­ìŠ¤: {len(names)} Ã— {len(names)}")
    
    mat = pd.DataFrame(np.nan, index=names, columns=names)
    for _, row in aai_df.iterrows():
        mat.at[row["Genome_A"], row["Genome_B"]] = row["AAI"]
        mat.at[row["Genome_B"], row["Genome_A"]] = row["AAI"]
    
    np.fill_diagonal(mat.values, 100)
    mat = mat.fillna(100)
    
    # ë§¤íŠ¸ë¦­ìŠ¤ ì €ì¥
    mat_file = os.path.join(output_dir, "aai_matrix.csv")
    mat.to_csv(mat_file)
    print(f"âœ“ ë§¤íŠ¸ë¦­ìŠ¤ ì €ì¥: {mat_file}")
    
    # íˆíŠ¸ë§µ
    print("\nğŸ¨ íˆíŠ¸ë§µ ìƒì„±...")
    try:
        plt.figure(figsize=(max(12, len(names)*0.3), max(10, len(names)*0.3)))
        sns.heatmap(mat, cmap='viridis', square=True, cbar_kws={'label': 'AAI (%)'})
        plt.title("Lactobacillus plantarum AAI Heatmap", fontsize=16, pad=20)
        plt.tight_layout()
        
        heatmap_file = os.path.join(output_dir, "aai_heatmap.png")
        plt.savefig(heatmap_file, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ“ íˆíŠ¸ë§µ: {heatmap_file}")
    except Exception as e:
        print(f"âœ— íˆíŠ¸ë§µ ì‹¤íŒ¨: {str(e)}")
    
    # ë´ë“œë¡œê·¸ë¨
    print("\nğŸŒ³ ë´ë“œë¡œê·¸ë¨ ìƒì„±...")
    try:
        dist = 100 - mat.values
        np.fill_diagonal(dist, 0)
        Z = linkage(squareform(dist, checks=False), method='average')
        
        plt.figure(figsize=(max(16, len(names)*0.2), 8))
        dendrogram(Z, labels=mat.index, leaf_rotation=90, leaf_font_size=8)
        plt.title("Lactobacillus plantarum AAI Dendrogram", fontsize=16, pad=20)
        plt.ylabel("Distance (100 - AAI%)", fontsize=12)
        plt.tight_layout()
        
        dendro_file = os.path.join(output_dir, "aai_dendrogram.png")
        plt.savefig(dendro_file, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ“ ë´ë“œë¡œê·¸ë¨: {dendro_file}")
    except Exception as e:
        print(f"âœ— ë´ë“œë¡œê·¸ë¨ ì‹¤íŒ¨: {str(e)}")
    
    mark_step_done(step_name, output_dir)
    return mat

# ========== 6ë‹¨ê³„: Low ANI í•„í„°ë§ ==========
def filter_low_ani(mat, output_dir):
    """Low ANI í•„í„°ë§"""
    step_name = "06_filter_low_ani"
    if is_step_done(step_name, output_dir):
        return
    
    print("\n" + "="*70)
    print(f"[6ë‹¨ê³„] Low ANI í•„í„°ë§ (< {ANI_THRESHOLD}%)")
    print("="*70)
    
    low_pairs = []
    for i in mat.index:
        for j in mat.columns:
            if i < j and mat.at[i, j] < ANI_THRESHOLD:
                low_pairs.append({"Genome_A": i, "Genome_B": j, "AAI": mat.at[i, j]})
    
    low_file = os.path.join(output_dir, "low_ani_filtered.tsv")
    
    if low_pairs:
        df_low = pd.DataFrame(low_pairs).sort_values("AAI")
        df_low.to_csv(low_file, sep="\t", index=False)
        print(f"\nâœ“ Low ANI ìŒ: {len(df_low)}ê°œ")
        print(f"ğŸ“„ ì €ì¥: {low_file}")
        for _, row in df_low.head(5).iterrows():
            print(f"   - {row['Genome_A']}: {row['Genome_B']}: {row['AAI']:.2f}%")
    else:
        pd.DataFrame(columns=["Genome_A", "Genome_B", "AAI"]).to_csv(low_file, sep="\t", index=False)
        print(f"\nâœ“ Low ANI ìŒ ì—†ìŒ (ëª¨ë‘ {ANI_THRESHOLD}% ì´ìƒ)")
    
    mark_step_done(step_name, output_dir)

# ========== 7ë‹¨ê³„: ìë™ ë©”íƒ€ë°ì´í„° ìƒì„± ==========
def generate_metadata(base_dir, output_dir, matched_df):
    """íŒŒì¼ëª… ê¸°ë°˜ ë©”íƒ€ë°ì´í„° ìë™ ìƒì„±"""
    step_name = "07_metadata"
    if is_step_done(step_name, output_dir):
        meta_file = os.path.join(output_dir, "metadata.tsv")
        if os.path.exists(meta_file):
            return pd.read_csv(meta_file, sep="\t")
        return None
    
    print("\n" + "="*70)
    print("[7ë‹¨ê³„] ìë™ ë©”íƒ€ë°ì´í„° ìƒì„±")
    print("="*70)
    
    # íŒŒì¼ëª…ì—ì„œ ì •ë³´ ì¶”ì¶œ
    metadata = []
    for _, row in matched_df.iterrows():
        gcf_id = row['GCF_ID']
        fna_file = row['FNA_file']
        
        # GCF ID íŒŒì‹±
        parts = gcf_id.split('_')
        assembly_version = parts[1] if len(parts) > 1 else "unknown"
        
        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata.append({
            "Sample": gcf_id,
            "FNA_File": fna_file,
            "FAA_File": row['FAA_file'],
            "Assembly_Version": assembly_version,
            "Species": "Lactobacillus plantarum",
            "Source": "NCBI RefSeq",
            "Created_Date": time.strftime("%Y-%m-%d")
        })
    
    meta_df = pd.DataFrame(metadata)
    meta_file = os.path.join(output_dir, "metadata.tsv")
    meta_df.to_csv(meta_file, sep="\t", index=False)
    
    print(f"âœ“ ë©”íƒ€ë°ì´í„° ìƒì„±: {len(meta_df)}ê°œ ìƒ˜í”Œ")
    print(f"ğŸ“„ ì €ì¥: {meta_file}")
    
    # ìš”ì•½
    try:
        meta_sum = meta_df.describe(include="all").T
        meta_sum.to_csv(os.path.join(output_dir, "metadata_summary.tsv"), sep="\t")
        print(f"âœ“ ë©”íƒ€ë°ì´í„° ìš”ì•½: metadata_summary.tsv")
    except Exception as e:
        print(f"âš ï¸  ìš”ì•½ ì‹¤íŒ¨: {str(e)}")
    
    mark_step_done(step_name, output_dir)
    return meta_df

# ========== 8ë‹¨ê³„: ìë™ MLST ë¶„ì„ (ì•ˆì „ ë²„ì „) ==========
def run_mlst_analysis(base_dir, output_dir, work_dir):
    """MLST ë„êµ¬ë¡œ ìë™ ë¶„ì„ (ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰)"""
    step_name = "08_mlst"
    if is_step_done(step_name, output_dir):
        mlst_file = os.path.join(output_dir, "mlst_results.tsv")
        if os.path.exists(mlst_file):
            return pd.read_csv(mlst_file, sep="\t")
        return None
    
    # MLST ë„êµ¬ í™•ì¸ (ì—†ìœ¼ë©´ ì²´í¬í¬ì¸íŠ¸ë§Œ ìƒì„±í•˜ê³  ë¦¬í„´)
    mlst_path = find_mlst()
    if not mlst_path:
        print("\nâš ï¸  MLST ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤")
        mark_step_done(step_name, output_dir)
        return None
    
    print("\n" + "="*70)
    print("[8ë‹¨ê³„] ìë™ MLST ë¶„ì„")
    print("="*70)
    
    # FAA íŒŒì¼ í™•ì¸
    if not os.path.exists(work_dir):
        print(f"âŒ ì‘ì—… ë””ë ‰í† ë¦¬ ì—†ìŒ: {work_dir}")
        mark_step_done(step_name, output_dir)
        return None
    
    fasta_files = [f for f in os.listdir(work_dir) if f.endswith('.fasta')]
    if not fasta_files:
        print("âŒ ë¶„ì„í•  FAA íŒŒì¼ ì—†ìŒ")
        mark_step_done(step_name, output_dir)
        return None
    
    # MLST ì‹¤í–‰
    mlst_results = []
    print(f"\nğŸ§¬ MLST ë¶„ì„ ì‹œì‘ ({len(fasta_files)}ê°œ íŒŒì¼)...")
    
    for idx, fname in enumerate(fasta_files, 1):
        fpath = os.path.join(work_dir, fname)
        prefix = fname.replace('.fasta', '')
        
        try:
            cmd = ['mlst', '--scheme', MLST_SCHEME, '--quiet', fpath]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            
            if result.returncode == 0:
                # ê²°ê³¼ íŒŒì‹±
                parts = result.stdout.strip().split('\t')
                if len(parts) >= 3:
                    mlst_results.append({
                        "Sample": prefix,
                        "Scheme": parts[0],
                        "ST": parts[1],
                        "Allele_Profile": "\t".join(parts[2:])
                    })
                    print(f"   [{idx}/{len(fasta_files)}] âœ“ {prefix}: ST={parts[1]}")
                else:
                    print(f"   [{idx}/{len(fasta_files)}] âš ï¸  {prefix}: í˜•ì‹ ì˜¤ë¥˜")
            else:
                print(f"   [{idx}/{len(fasta_files)}] âš ï¸  {prefix}: ì‹¤íŒ¨ (ë°˜í™˜ì½”ë“œ: {result.returncode})")
                
        except subprocess.TimeoutExpired:
            print(f"   [{idx}/{len(fasta_files)}] âš ï¸  {prefix}: íƒ€ì„ì•„ì›ƒ (120ì´ˆ)")
        except Exception as e:
            print(f"   [{idx}/{len(fasta_files)}] âš ï¸  {prefix}: {str(e)}")
    
    # ê²°ê³¼ ì €ì¥
    if mlst_results:
        mlst_df = pd.DataFrame(mlst_results)
        mlst_file = os.path.join(output_dir, "mlst_results.tsv")
        mlst_df.to_csv(mlst_file, sep="\t", index=False)
        
        print(f"\nâœ“ MLST ì™„ë£Œ: {len(mlst_results)}ê°œ ìƒ˜í”Œ")
        print(f"ğŸ“„ ì €ì¥: {mlst_file}")
        
        # ST ìš”ì•½
        st_counts = mlst_df['ST'].value_counts()
        st_summary_file = os.path.join(output_dir, "st_summary.tsv")
        st_counts.to_csv(st_summary_file, sep="\t", header=['Count'])
        print(f"ğŸ“„ ST ìš”ì•½: {st_summary_file}")
        
        # STë³„ ìƒ˜í”Œ ëª©ë¡
        st_samples = mlst_df.groupby('ST')['Sample'].apply(list)
        st_samples_file = os.path.join(output_dir, "st_samples.tsv")
        with open(st_samples_file, 'w') as f:
            f.write("ST\tSamples\tCount\n")
            for st, samples in st_samples.items():
                f.write(f"{st}\t{','.join(samples)}\t{len(samples)}\n")
        print(f"ğŸ“„ STë³„ ìƒ˜í”Œ: {st_samples_file}")
        
    else:
        print("\nâš ï¸  MLST ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    mark_step_done(step_name, output_dir)
    return mlst_df if mlst_results else None

# ========== ë©”ì¸ ì‹¤í–‰ ==========
def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("\n" + "="*70)
    print("  Lactobacillus plantarum AAI ë¶„ì„ íŒŒì´í”„ë¼ì¸")
    print("  (í™˜ê²½ ì¶©ëŒ í•´ê²°: MLST ì„ íƒì  ì‹¤í–‰)")
    print("="*70)
    print(f"  Python: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}")
    print(f"  MLST ìŠ¤í‚´: {MLST_SCHEME}")
    print(f"  ANI ì„ê³„ê°’: {ANI_THRESHOLD}%")
    print(f"  MLST ë„êµ¬: {'ì‚¬ìš©' if MLST_TOOL else 'ë¹„í™œì„±í™”'}")
    print("="*70)
    
    start_time = time.time()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    fna_dir, faa_dir = get_working_paths()
    output_dir = os.path.join(fna_dir, OUTPUT_NAME)
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\nğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    try:
        # 1ë‹¨ê³„: ë§¤ì¹­
        matched_df, _, _ = match_prokka_fna(output_dir)
        if matched_df is None:
            print("\nâŒ ë§¤ì¹­ ì‹¤íŒ¨ë¡œ ì¢…ë£Œ")
            sys.exit(1)
        
        work_dir = os.path.join(faa_dir, "matched_prokka_faa")
        
        # 2ë‹¨ê³„: íŒŒì¼ ë³µì‚¬
        work_dir = prepare_faa_working_directory(matched_df, faa_dir, fna_dir)
        if work_dir is None:
            print("\nâŒ íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨")
            sys.exit(1)
        
        # 3ë‹¨ê³„: CompareM ì°¾ê¸°
        comparem_path = find_comparem()
        if not comparem_path:
            print("\nâŒ CompareM ì—†ìŒ")
            sys.exit(1)
        
        # 4ë‹¨ê³„: AAI ë¶„ì„
        success, work_dir = group_aai_analysis(comparem_path, work_dir, output_dir)
        if not success:
            print("\nâš ï¸  AAI ë¶„ì„ ë¶€ë¶„ ì‹¤íŒ¨, ê²°ê³¼ í†µí•© ì‹œë„")
        
        # 5ë‹¨ê³„: ê²°ê³¼ í†µí•©
        aai_df = merge_aai_results(work_dir, output_dir)
        if aai_df is None:
            print("\nâŒ í†µí•©í•  ê²°ê³¼ ì—†ìŒ")
            sys.exit(1)
        
        # 6ë‹¨ê³„: ì‹œê°í™”
        mat = build_matrix_and_visualize(aai_df, output_dir)
        
        # 7ë‹¨ê³„: Low ANI í•„í„°ë§
        filter_low_ani(mat, output_dir)
        
        # 8ë‹¨ê³„: ìë™ ë©”íƒ€ë°ì´í„° ìƒì„±
        generate_metadata(fna_dir, output_dir, matched_df)
        
        # 9ë‹¨ê³„: ìë™ MLST ë¶„ì„ (ì„ íƒì )
        if MLST_TOOL:
            try:
                mlst_result = run_mlst_analysis(fna_dir, output_dir, work_dir)
                if mlst_result is None:
                    print("\nâš ï¸  MLST ê²°ê³¼ ì—†ìŒ â†’ ì£¼ ë¶„ì„ì€ ì •ìƒ ì™„ë£Œ")
            except Exception as e:
                print(f"\nâš ï¸  MLST ë¶„ì„ ì˜¤ë¥˜ (ë¬´ì‹œ): {str(e)}")
        
        # ì™„ë£Œ ìš”ì•½
        elapsed = time.time() - start_time
        mins, secs = divmod(int(elapsed), 60)
        
        print("\n" + "="*70)
        print("  ğŸ‰ ë¶„ì„ ì™„ë£Œ!")
        print("="*70)
        print(f"\nâ±ï¸  ì‹¤í–‰ ì‹œê°„: {mins}ë¶„ {secs}ì´ˆ")
        print(f"ğŸ“ ê²°ê³¼ ìœ„ì¹˜: {output_dir}")
        print(f"\nğŸ“Š ì²˜ë¦¬ ìƒ˜í”Œ: {len(matched_df)}ê°œ")
        
        print(f"\n{'â”€'*70}")
        print("ğŸ“‚ ìƒì„±ëœ í•µì‹¬ íŒŒì¼")
        print(f"{'â”€'*70}")
        
        core_files = [
            "matched_samples.tsv",
            "aai_summary_merged.tsv",
            "aai_matrix.csv",
            "aai_heatmap.png",
            "aai_dendrogram.png",
            "metadata.tsv"
        ]
        
        for cf in core_files:
            full_path = os.path.join(output_dir, cf)
            if os.path.exists(full_path):
                print(f"   âœ“ {cf}")
            else:
                print(f"   âœ— {cf}")
        
        print(f"\n{'â”€'*70}")
        print("ğŸ”§ MLST ê´€ë ¨ (ì„ íƒì )")
        print(f"{'â”€'*70}")
        print(f"   âœ“ low_ani_filtered.tsv")
        if MLST_TOOL and os.path.exists(os.path.join(output_dir, "mlst_results.tsv")):
            print(f"   âœ“ mlst_results.tsv")
            print(f"   âœ“ st_summary.tsv")
        else:
            print(f"   âš  mlst_results.tsv (MLST ë„êµ¬ ì—†ìŒ)")
            print(f"\nğŸ’¡ MLST ë¶„ì„ì„ ì›í•œë‹¤ë©´:")
            print(f"   conda create -n mlst_env python=3.11 mlst")
            print(f"   conda activate mlst_env")
            print(f"   cd {work_dir}")
            print(f"   mlst --scheme {MLST_SCHEME} *.fasta > {os.path.join(output_dir, 'mlst_results.tsv')}")
        
        print(f"\n{'â”€'*70}")
        print("âœ… ì£¼ ë¶„ì„ (AAI) ì •ìƒ ì™„ë£Œ!")
        print("="*70)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ì ì¤‘ë‹¨")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
